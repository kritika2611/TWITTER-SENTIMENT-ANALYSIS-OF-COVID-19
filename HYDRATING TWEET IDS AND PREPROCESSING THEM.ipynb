{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONVERTING CSV FILES INTO TXT FOR INPUT TO TWARC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('IEEE dataset/ORIGINAL DATA/april28-may29.csv',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46059, 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ids=[int(i) for i in data[:,0]]\n",
    "ids=np.asarray(ids)\n",
    "sent_score=np.asarray(data[:,1])\n",
    "df=list(zip(ids,sent_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1254995208888094720, 0.0),\n",
       " (1254995485452050432, 0.0),\n",
       " (1254995705527177216, 0.525),\n",
       " (1254995868333289472, 0.28750000000000003),\n",
       " (1254996123514789888, 0.0),\n",
       " (1254996305082036224, 0.313973063973064),\n",
       " (1254996436162277376, -0.05),\n",
       " (1254996786105790464, 0.0),\n",
       " (1254997859285508096, 0.0),\n",
       " (1254998021722390528, 0.0)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=dict([key,value] for (key,value) in df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46037"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file=open('IEEE dataset/TWEET IDS/april28-may29.txt','w',encoding='utf8')\n",
    "for i in ids:\n",
    "    print((i),file=out_file)\n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MY PREPROCESSOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* LowerCasing- The tweet is first converted into lower case\n",
    "* Removal of Retweet|RT|via\n",
    "* Removal of mentions(like @xyz)\n",
    "* Removal of links in the tweet\n",
    "* Negation handling\n",
    "* Tokenisation to get standard english words only\n",
    "* Stop word removal - the stopwords were modified\n",
    "* Lemmatization using WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok=RegexpTokenizer(r'[a-zA-Z]+')     #taking only alphabets\n",
    "sw=stopwords.words(\"english\")\n",
    "lm=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw.remove('not')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw.remove('no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "177"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "negations=[\"isn't\",\"aren't\",\"wasn't\",\"weren't\",\"won't\", \"wouldn't\",\"ain't\",\"doesn't\",\"don't\",\"didn't\", \"can't\",\"couldn't\", \"doesn't\", \"hadn't\", \"hasn't\", \"haven't\", \"mightn't\", \"mustn't\", \"needn't\", \"shan't\", \"shouldn't\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_data(text):\n",
    "    text=text.lower()\n",
    "    #removing retweets\n",
    "    text=re.sub(r'(rt|RT|retweet|from|via)',\"\",text)\n",
    "    \n",
    "    #removing mentions \n",
    "    mentions=re.findall(\"@[A-Za-z0-9_]+\",text)\n",
    "    for m in mentions:\n",
    "        text=text.replace(m,\"\")\n",
    "        \n",
    "    #links removal\n",
    "    urls=re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', text)\n",
    "    for u in urls:\n",
    "        text=text.replace(u,\"\")\n",
    "        \n",
    "    #handling negations- REPLACING ALL THE NEGATIVE HELPING VERBS WITH NOT\n",
    "    for w in negations:\n",
    "        if text.find(w)!=-1:\n",
    "            text=text.replace(w,'not')\n",
    "            \n",
    "    #tokenisation\n",
    "    tokens=tok.tokenize(text)      #takes only standard english alphabets\n",
    "    #stopword removal\n",
    "    new_tokens=[t for t in tokens if t not in sw and len(t)>2]\n",
    "    #stemming\n",
    "    clean_text=[lm.lemmatize(t) for t in new_tokens]\n",
    "    \n",
    "    clean_text=\" \".join(clean_text)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'not worry pandemic end soon wash hand regular itervals not step outside without wearing mask seeing following symptom dry cough high fever breathing difficulty loss taste loss smell visit nearby hospital detail visit indiafightscorona'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_clean_data(\"\"\"RT@abc, @ghs: Don't worry, this pandemic will end soon\n",
    "Wash your hands at regular itervals, Do not a step outside without wearing a mask :)\n",
    "On seeing any of the following symptoms: dry cough, high fever, breathing difficulty, loss of taste, loss of smell, visit nearby hospital\n",
    "For more details visit:'http://tnp.dtu.ac.in/rm_2016-17/intern/intern_login,\n",
    " 'https://icmr.gov.in',\n",
    " 'http://www.google.com/egfbhsd/g\n",
    "#IndiafightsCorona\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'message universal stay home stay safe let defeat corona together day rediscovered farming ceo find nearby corona testing treatment amp isolation close president note seen crossed corona replaced chinese virus corona virus got texting people back lol corona day feel like sunday house intimidate corona deleted scene twd special corona virus captain never change'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_clean_data(\"\"\"RT @ashokgehlot51: This message is universal . Stay home, stay safe - let’s Defeat Corona together #राजस्थान_सतर्क_है\n",
    "RT @MakedaMorrison: Day 5: We have rediscovered farming\n",
    "RT @_rohanverma: I am the CEO of @MapmyIndia - through https://t.co/p8Iqtoz77t all can find nearby corona testing, treatment &amp; isolation ce…\n",
    "RT @jabinbotsford: Close up of President @realDonaldTrump notes is seen where he crossed out \"Corona\" and replaced it with \"Chinese\" Virus…\n",
    "This corona virus got me texting people back now lol.\n",
    "RT @Biancaixvi: Corona day 3: it just feels like Sunday again and...again\n",
    "house. he will intimidate the corona https://t.co/KXrYmAS25H\n",
    "RT @chandlerriggs: here’s a deleted scene from TWD’s special on corona virus https://t.co/qGuSfyjpK5\n",
    "RT @suliceu99: our captain never change :\") https://t.co/47gGJdiIvs\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HYDRATING IDS USING TWARC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twarc import Twarc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twitter_credentials import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=Twarc(CONSUMER_KEY,CONSUMER_SECRET,ACCESS_TOKEN,ACESS_TOKEN_SECRET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "twarc.client.Twarc"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "processed_tweet=open('IEEE dataset/PROCESSED DATA/new_april28-may29.txt','w',encoding='utf8')\n",
    "for tweet in t.hydrate(open('IEEE dataset/TWEET IDS/april28-may29.txt')):\n",
    "    txt=str(tweet['full_text'])\n",
    "    t_id=int(tweet['id'])\n",
    "    sent=d[t_id]\n",
    "    clean_txt=get_clean_data(txt)\n",
    "    #if clean_txt!=None:\n",
    "    print((sent),end=\" \",file=processed_tweet)\n",
    "    print((clean_txt),file=processed_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_tweet.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_tweet=open('IEEE dataset/PROCESSED DATA/new_april28-may29.txt','r',encoding='utf8')\n",
    "lines=processed_tweet.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22493"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
